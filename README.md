# Data_Science_GAZE_Detection-controller
Absolutely! Here's a professional, well-structured documentation for your MobileNetV2 gaze classification model and training process â€” ideal for reports, presentations, or GitHub README.
________________________________________
ğŸ“„ Gaze Direction Classification using MobileNetV2
ğŸ“Œ Project Overview
This project implements a gaze direction classification system as part of a gaze-controlled robotic car. The model takes in cropped right-eye images and classifies them into one of four gaze directions:
â€¢	Left
â€¢	Right
â€¢	Forward
â€¢	Close (eyes closed â†’ stop command)
The trained model is intended for real-time deployment on a Raspberry Pi 4 to drive a robot based on eye movements, enhancing accessibility and assistive control.
________________________________________
ğŸ§  Model: MobileNetV2 (Transfer Learning)
ğŸ“¦ Architecture
â€¢	Base model: MobileNetV2
â€¢	Input shape: 224x224x3
â€¢	Top layers added:
o	GlobalAveragePooling2D
o	Dropout (0.3)
o	Dense(4, activation='softmax') for 4-class classification
ğŸ” Rationale for MobileNetV2
â€¢	Lightweight and optimized for mobile/edge devices
â€¢	Strong feature extraction with low latency
â€¢	Easily converted to TensorFlow Lite for Raspberry Pi deployment
________________________________________
ğŸ”§ Dataset
â€¢	Collected using: MediaPipe FaceMesh (right-eye crop)
â€¢	Classes: left_eye, right_eye, forward_eye, close_eye
â€¢	Subjects: 20 people Ã— ~50 images per class = ~4000+ images
â€¢	Image size: 224Ã—224
â€¢	Split method: Person-wise 4-fold cross-validation (14 train / 3 val / 3 test per fold)
________________________________________
ğŸ—ï¸ Training Strategy
âœ… Preprocessing
â€¢	Images rescaled to [0, 1]
â€¢	Augmentations applied only to training:
o	rotation_range = 15Â°
o	zoom_range = 0.2
o	brightness_range = [0.6, 1.4]
o	width_shift / height_shift = 0.1
âœ… Training Phases
Phase 1: Classification Head
â€¢	Base MobileNetV2 frozen
â€¢	Optimizer: Adam (1e-4)
â€¢	Loss: Categorical Crossentropy + Label Smoothing (0.1)
â€¢	Epochs: 50
â€¢	Scheduler: Cosine decay learning rate
â€¢	EarlyStopping on val_loss
Phase 2: Fine-Tuning
â€¢	Unfreeze top 30 layers of MobileNetV2
â€¢	Learning Rate: 2e-5
â€¢	Optimizer: Adam
â€¢	Additional 50 epochs
â€¢	Same callbacks
________________________________________
ğŸ“Š Performance Summary
Fold	Val Acc	Test Acc	F1 Score
1	0.9059	0.8425	0.8427
2 âœ…	0.8286	0.9077	0.9052
3	0.8398	0.7866	0.7447
4	0.9493	0.8387	0.8386
â€¢	Average Test Accuracy: 84.39% Â± 4.3%
â€¢	Best Model: model_fold_2_fine_tuned.keras
________________________________________
ğŸ“¤ Deployment
â€¢	Converted to .tflite using TensorFlow Lite converter
â€¢	Optimized with:
o	float32 and float16 quantization
â€¢	Real-time inference on Raspberry Pi 4 with GPIO control for robot steering
________________________________________
ğŸ“ Output Artifacts
â€¢	model_fold_X_phase1.keras
â€¢	model_fold_X_fine_tuned.keras
â€¢	classification_report_fold_X.txt
â€¢	confusion_matrix_fold_X.png
â€¢	tflite_models/model_fold_X_fine_tuned.tflite
________________________________________
Would you like me to generate this as:
â€¢	A Markdown file?
â€¢	A PDF report?
â€¢	Or embed it into your GitHub README?
Just say the format â€” and Iâ€™ll deliver it for you.

